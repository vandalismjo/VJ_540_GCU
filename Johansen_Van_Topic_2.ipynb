{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment includes a theoretical part and an application part. Demonstrate your understanding of the concepts discussed in this topic, research a technique, and implement it. \n",
    "\n",
    "#### Part 1: Theory \n",
    "\n",
    "1. What is the role of a confusion matrix in the evaluation of a machine trained for a pattern recognition task? In your answer, refer to a concrete example, either from literature or one you created. Anchor you answer in relevant literature. \n",
    "\n",
    "2. What is the role of the ROC curve? How would you use it to compare the performance of several classifiers? In your answer, refer to concrete examples of classifiers, either from literature or one you created. Illustrate the ROC curves and anchor your answer in relevant literature. \n",
    "\n",
    "#### Part 2: Application \n",
    "\n",
    "You are tasked to build an image classifier for the MNIST dataset of handwritten numbers, implementing the k-nearest neighbors (k-NN) algorithm. You will need the following: \n",
    "\n",
    "The MNIST dataset, available on multiple servers on the Internet. For example: \n",
    "\n",
    "- http://yann.lecun.com/exdb/mnist/ \n",
    "\n",
    "- http://www.pymvpa.org/datadb/mnist.html \n",
    "\n",
    "The Python package neighbors.KNeighborsClassifier:   \n",
    "\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html \n",
    "\n",
    "The input to your classifier program is an image containing a digit, 0-9. Your program must correctly identify the digit with an accuracy of 95%. Here the outline of your task, but you will have to do a bit of research on your own (and increasingly so throughout the program) to fill in the details: \n",
    "\n",
    "Familiarize yourself with the MNIST dataset \n",
    "\n",
    "Familiarize yourself with the k-NN algorithm and its Python implementation in sklearn \n",
    "\n",
    "Create a Jupyter notebook for this assignment and implement the k-NN algorithm: \n",
    "\n",
    "- Import the package kNeighborsClassifier. \n",
    "\n",
    "- Be mindful of the train-test split and set the parameters accordingly (justify your choice). \n",
    "\n",
    "- Identify the variables in the dataset and define the Euclidean distance between an element in the test set and the training set. \n",
    "\n",
    "- Calculate the distance between the test element and each of if its k nearest neighbors. \n",
    "\n",
    "- Count the occurrence of each digit within the k nearest neighbors and identify the most popular digit. \n",
    "\n",
    "- Identify the test element as the digit voted as most popular in the set of the k nearest neighbors. \n",
    "\n",
    "- Classify the test element accordingly (i.e. based on the popular vote). \n",
    "\n",
    "- Calculate the error. \n",
    "\n",
    "\n",
    "#### Part 3: Report\n",
    "\n",
    "Review the article “Handwritten Digit Recognition Using K-Nearest Neighbour Classifier.” Note the algorithms used, but focus on the way the authors: \n",
    "\n",
    "\n",
    "- Present the findings \n",
    "\n",
    "- Discuss the findings \n",
    "\n",
    "- Calculate the accuracy of the results \n",
    "\n",
    "- Write the article, using professional terminology and content organization \n",
    "\n",
    "Write a technical report (i.e., not a full-fledged academic paper) to accompany the Jupyter notebook that implements the classifier, using the aforementioned article as a guide on what to address and how to present the mini-project and report the findings. \n",
    "\n",
    "- Address the potential role of a confusion matrix in your report (refer to Part 1). \n",
    "\n",
    "- Address the potential role of ROC curve in your report (refer to Part 1). \n",
    "\n",
    "APA style is expected, as well as formal and rigorous scientific writing, using appropriate mathematical notation and references. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrices are fantastic tools that are used to evaluate the performance of a trained model. There are several scores that each give critical information that help inform the data scientist about which model performs better than the other. Each score gives critical information and depending on the objective of the model, each one can serve a different purpose. While the matrix lays out the true positives, true negatives, false positives, and false negatives the data scientist can calculate the following metrics: misclassification rate, success rate, true positive rate, true negative rate, sensitivity, specificity, precision, recall, f1 score, and the receiver operating characteristic (ROC) curve. With all the scores that come from the predicted values versus the actual values a data scientist can correctly assess the performance of the model given the business objective (Gopal, 2019, p. 71). \n",
    "\n",
    "\n",
    "One example is when predicting cancer patients, the classification predicts that a certain patient has cancer, so the doctor goes to him and his family and tells him about this and all the possibilities. However, if you find out you incorrectly classified him as having cancer when really, he does not you will have to tell him and his family who might be in shock. That’s why the data scientist wants the classifier 100% confident, or in more simpler words they want 100% precision. The worse situation in this example is classifying a patient that actually has cancer as not having cancer. They will then think they are safe but in reality, they would die soon. This is very bad situation, and to avoid it the data scientist would want to increase the recall.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The ROC curve uses information from the confusion matrix, the predicted values and the actual values, to generate the following information and scores. The ROC curve is on a two-dimensional graph which plots the true positive rate, sensitivity, on the y-axis and false positive rate, complement of the specificity, on the x-axis. A ROC curve shows relative trade-offs between advantages, true positives, and costs, false positives (Gopal, 2019, p. 75). When making decisions and comparisons with the ROC curve, the preferred plot will have points moving to the top left corner - true positive rate is higher while false positive rate is lower. The data scientist will also consider and make comparisons by calculating the area under this curve (McClish, 1989). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In k-NN classification, the output is a class. An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors. The k nearest neighbor (kNN) method is a popular classification method in data mining and statistics because of its simple implementation and significant classification performance. However, it is impractical for traditional kNN methods to assign a fixed k value (even though set by experts) to all test samples (Zhang, 2018)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, plot_roc_curve, roc_curve\n",
    "from skimage import exposure\n",
    "import imutils\n",
    "import cv2\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x24fbd3f5d48>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "image_index = 60 \n",
    "print(y_train[image_index])\n",
    "plt.imshow(x_train[image_index], cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data points: 1212\n",
      "validation data points: 135\n",
      "testing data points: 450\n"
     ]
    }
   ],
   "source": [
    "mnist = datasets.load_digits()\n",
    " \n",
    "# take the MNIST data and construct the training and testing split, using 75% of the\n",
    "# data for training and 25% for testing\n",
    "(trainData, testData, trainLabels, testLabels) = train_test_split(np.array(mnist.data), mnist.target, \n",
    "                                                                  test_size=0.25, random_state=42)\n",
    " \n",
    "# Take 10% of the training data and use that for validation\n",
    "(trainData, valData, trainLabels, valLabels) = train_test_split(trainData, trainLabels, \n",
    "                                                                test_size=0.1, random_state=84)\n",
    " \n",
    "# show the sizes of each data split\n",
    "print(\"training data points: {}\".format(len(trainLabels)))\n",
    "print(\"validation data points: {}\".format(len(valLabels)))\n",
    "print(\"testing data points: {}\".format(len(testLabels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=1, accuracy=99.26%\n",
      "k=3, accuracy=99.26%\n",
      "k=5, accuracy=99.26%\n",
      "k=7, accuracy=99.26%\n",
      "k=9, accuracy=99.26%\n",
      "k=11, accuracy=99.26%\n",
      "k=13, accuracy=99.26%\n",
      "k=15, accuracy=99.26%\n",
      "k=17, accuracy=98.52%\n",
      "k=19, accuracy=98.52%\n",
      "k=21, accuracy=97.78%\n",
      "k=23, accuracy=97.04%\n",
      "k=25, accuracy=97.78%\n",
      "k=27, accuracy=97.04%\n",
      "k=29, accuracy=97.04%\n",
      "k=1 achieved highest accuracy of 99.26% on validation data\n"
     ]
    }
   ],
   "source": [
    "# initialize the values of k for our k-Nearest Neighbor classifier along with the list of \n",
    "# accuracies for each value of k\n",
    "kVals = range(1, 30, 2)\n",
    "accuracies = []\n",
    " \n",
    "# loop over various values of `k` for the k-Nearest Neighbor classifier\n",
    "for k in range(1, 30, 2):\n",
    "    # train the k-Nearest Neighbor classifier with the current value of `k`\n",
    "    model = KNeighborsClassifier(n_neighbors=k)\n",
    "    model.fit(trainData, trainLabels)\n",
    "\n",
    "    # evaluate the model and update the accuracies list\n",
    "    score = model.score(valData, valLabels)\n",
    "    print(\"k=%d, accuracy=%.2f%%\" % (k, score * 100))\n",
    "    accuracies.append(score)\n",
    "\n",
    "# find the value of k that has the largest accuracy\n",
    "i = int(np.argmax(accuracies))\n",
    "print(\"k=%d achieved highest accuracy of %.2f%% on validation data\" % (kVals[i], accuracies[i] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION ON TESTING DATA\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        43\n",
      "           1       0.95      1.00      0.97        37\n",
      "           2       1.00      1.00      1.00        38\n",
      "           3       0.98      0.98      0.98        46\n",
      "           4       0.98      0.98      0.98        55\n",
      "           5       0.98      1.00      0.99        59\n",
      "           6       1.00      1.00      1.00        45\n",
      "           7       1.00      0.98      0.99        41\n",
      "           8       0.97      0.95      0.96        38\n",
      "           9       0.96      0.94      0.95        48\n",
      "\n",
      "    accuracy                           0.98       450\n",
      "   macro avg       0.98      0.98      0.98       450\n",
      "weighted avg       0.98      0.98      0.98       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# re-train the classifier using the best k value and predict the labels of the\n",
    "# test data\n",
    "model = KNeighborsClassifier(n_neighbors=kVals[i])\n",
    "model = KNeighborsClassifier(n_neighbors=1)\n",
    "model.fit(trainData, trainLabels)\n",
    "predictions = model.predict(testData)\n",
    " \n",
    "# show a final classification report demonstrating the accuracy of the classifier\n",
    "# for each of the digits\n",
    "print(\"EVALUATION ON TESTING DATA\")\n",
    "print(classification_report(testLabels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  99.25925925925925\n",
      "error =  0.7407407407407418\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model and update the accuracies list\n",
    "score = model.score(valData, valLabels)\n",
    "print(\"accuracy = \", (score * 100))\n",
    "print(\"error = \", (1 - score)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think that digit is: 2\n"
     ]
    }
   ],
   "source": [
    "# validate \n",
    "for i in list(map(int, np.random.randint(0, high=len(testLabels), size=(1,)))):\n",
    "    # grab the image and classify it\n",
    "    image = testData[i]\n",
    "    prediction = model.predict(image.reshape(1, -1))[0]\n",
    " \n",
    "    # convert the image for a 64-dim array to an 8 x 8 image compatible with OpenCV,\n",
    "    # then resize it to 32 x 32 pixels so we can see it better\n",
    "    image = image.reshape((8, 8)).astype(\"uint8\")\n",
    "    image = exposure.rescale_intensity(image, out_range=(0, 255))\n",
    "    image = imutils.resize(image, width=32, inter=cv2.INTER_CUBIC)\n",
    " \n",
    "    # show the prediction\n",
    "    print(\"I think that digit is: {}\".format(prediction))\n",
    "    cv2.imshow(\"Image\", image)\n",
    "    cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see attached image for result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[43  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 37  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 38  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 45  0  0  0  0  1  0]\n",
      " [ 0  1  0  0 54  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 59  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 45  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 40  0  1]\n",
      " [ 0  1  0  0  0  0  0  0 36  1]\n",
      " [ 0  0  0  1  1  1  0  0  0 45]]\n"
     ]
    }
   ],
   "source": [
    "# construct confusion matrix\n",
    "print(confusion_matrix(testLabels, predictions))\n",
    "\n",
    "# all results and analysis attached with part 3 below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 \n",
    "\n",
    "Attached separately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
